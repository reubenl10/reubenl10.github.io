<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Polynomial Regression from Scratch | Reuben Lopez</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #ffffff;
        }

        nav {
            background-color: #1a2332;
            padding: 1rem 0;
            position: sticky;
            top: 0;
            z-index: 1000;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        nav ul {
            display: flex;
            justify-content: center;
            list-style: none;
            flex-wrap: wrap;
        }

        nav a {
            color: #ffffff;
            text-decoration: none;
            padding: 0.5rem 1.5rem;
            transition: color 0.3s;
            font-weight: 500;
        }

        nav a:hover {
            color: #14b8a6;
        }

        .post-header {
            background: linear-gradient(135deg, #1a2332 0%, #2d3e50 100%);
            color: #ffffff;
            padding: 5rem 2rem 4rem;
            text-align: center;
        }

        .post-header .category {
            color: #14b8a6;
            font-size: 0.85rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.1em;
            margin-bottom: 1rem;
        }

        .post-header h1 {
            font-size: 2.5rem;
            font-weight: 700;
            margin-bottom: 1rem;
            max-width: 800px;
            margin-left: auto;
            margin-right: auto;
            line-height: 1.3;
        }

        .post-header .meta {
            color: #cbd5e1;
            font-size: 0.95rem;
        }

        .post-body {
            max-width: 740px;
            margin: 0 auto;
            padding: 4rem 2rem;
        }

        .post-body p {
            color: #475569;
            font-size: 1.05rem;
            line-height: 1.85;
            margin-bottom: 1.5rem;
        }

        .post-body h2 {
            color: #1a2332;
            font-size: 1.6rem;
            font-weight: 700;
            margin: 2.5rem 0 1rem;
        }

        .post-body h3 {
            color: #1a2332;
            font-size: 1.2rem;
            font-weight: 600;
            margin: 2rem 0 0.75rem;
        }

        .post-body ul, .post-body ol {
            color: #475569;
            font-size: 1.05rem;
            line-height: 1.85;
            margin-bottom: 1.5rem;
            padding-left: 1.5rem;
        }

        .post-body li {
            margin-bottom: 0.4rem;
        }

        pre {
            background: #1a2332;
            color: #e2e8f0;
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin: 1.5rem 0 2rem;
            font-size: 0.9rem;
            line-height: 1.6;
        }

        code {
            font-family: 'SF Mono', 'Fira Code', 'Consolas', monospace;
        }

        p code, li code {
            background: #f1f5f9;
            color: #0d9488;
            padding: 0.15em 0.4em;
            border-radius: 4px;
            font-size: 0.9em;
        }

        .callout {
            background: #f0fdfa;
            border-left: 4px solid #14b8a6;
            padding: 1.25rem 1.5rem;
            border-radius: 0 8px 8px 0;
            margin: 1.5rem 0 2rem;
            color: #134e4a;
            font-size: 1rem;
            line-height: 1.7;
        }

        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.4rem;
            color: #14b8a6;
            text-decoration: none;
            font-weight: 600;
            font-size: 0.95rem;
            margin-bottom: 2rem;
        }

        .back-link:hover {
            color: #0d9488;
        }

        footer {
            background: #1a2332;
            color: #94a3b8;
            text-align: center;
            padding: 2rem;
            font-size: 0.9rem;
            margin-top: 4rem;
        }

        @media (max-width: 768px) {
            .post-header h1 { font-size: 1.8rem; }
            .post-body { padding: 2.5rem 1.25rem; }
        }
    </style>
</head>
<body>

    <nav>
        <ul>
            <li><a href="../index.html#about">About</a></li>
            <li><a href="../index.html#experience">Experience</a></li>
            <li><a href="../index.html#projects">Projects</a></li>
            <li><a href="../index.html#blog">Blog</a></li>
            <li><a href="../index.html#contact">Contact</a></li>
        </ul>
    </nav>

    <header class="post-header">
        <div class="category">Machine Learning · From Scratch</div>
        <h1>Polynomial Regression from Scratch</h1>
        <div class="meta">Reuben Lopez &nbsp;·&nbsp; January 2021</div>
    </header>

    <article class="post-body">
        <a href="../index.html#blog" class="back-link">← Back to Blog</a>

        <p>
            Linear regression is a great starting point, but the real world is rarely linear. When I ran my first regression on curved data, the model produced a flat line that missed the pattern entirely. Polynomial regression was the fix — and understanding why it works revealed something non-obvious about how "linear" models actually work.
        </p>

        <h2>The Problem: Linear Models on Curved Data</h2>
        <p>
            To make this concrete, I generated 200 data points using the formula:
        </p>
        <pre><code>y = 0.5 * X**2 + X + 2 + noise</code></pre>
        <p>
            The underlying relationship is quadratic. A scatter plot makes this obvious — the data bends upward in a parabola. If you fit a straight line to this, it cuts through the middle and leaves systematic error on both ends. The residuals aren't random noise; they have structure, which means your model is missing something real.
        </p>

        <h2>The Key Insight: Feature Transformation</h2>
        <p>
            Here's the non-obvious part: polynomial regression isn't a fundamentally different algorithm. It's just linear regression applied to transformed features.
        </p>
        <p>
            Instead of fitting <code>y = a*X + b</code>, you add a new column to your feature matrix: <code>X²</code>. Now you're fitting <code>y = a*X² + b*X + c</code>, which is linear in the parameters even though it's nonlinear in X. The estimation problem is still the same — you're just working in a higher-dimensional feature space.
        </p>

        <div class="callout">
            Polynomial regression fits a nonlinear model to the data, but it remains a linear statistical estimation problem. The "linear" in linear regression refers to linearity in the parameters, not linearity in the input features.
        </div>

        <h2>The Implementation</h2>
        <p>
            The pipeline has two steps: transform the features, then fit a standard linear regression.
        </p>

        <pre><code>import numpy as np
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

# Generate nonlinear data
np.random.seed(42)
X = 6 * np.random.rand(200, 1) - 3
y = 0.5 * X**2 + X + 2 + np.random.randn(200, 1)

# Step 1: Transform features (add X^2 column)
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_features.fit_transform(X)
# X_poly now has columns: [X, X^2]

# Step 2: Fit linear regression on transformed features
lin_reg = LinearRegression()
lin_reg.fit(X_poly, y)

print(lin_reg.coef_)    # Should be close to [1, 0.5]
print(lin_reg.intercept_)  # Should be close to [2]</code></pre>

        <p>
            The coefficients recovered should closely match the true values I used to generate the data: ~1 for X, ~0.5 for X², and ~2 for the intercept.
        </p>

        <h2>Writing a Custom Prediction Function</h2>
        <p>
            To make predictions on new values without going through the full pipeline, I wrote a simple helper using the learned coefficients directly:
        </p>

        <pre><code>def poly_predict(x, coef, intercept):
    return x**2 * coef[1] + x * coef[0] + intercept

# Predict for a grid of x values to plot the fitted curve
X_plot = np.linspace(-3, 3, 300).reshape(-1, 1)
y_plot = poly_predict(X_plot, lin_reg.coef_[0], lin_reg.intercept_[0])</code></pre>

        <p>
            Overlaying the fitted curve on the scatter plot confirmed the model captured the parabolic shape cleanly.
        </p>

        <h2>Choosing the Right Degree</h2>
        <p>
            Degree is a critical hyperparameter. Higher degree polynomials can fit increasingly complex curves, but they also overfit more aggressively:
        </p>
        <ul>
            <li><strong>Degree 1</strong> — plain linear regression; underfit on curved data</li>
            <li><strong>Degree 2</strong> — captures quadratic relationships; good fit for this dataset</li>
            <li><strong>Degree 10+</strong> — can fit the training points exactly but produces wild oscillations on new data</li>
        </ul>
        <p>
            In practice you'd use cross-validation to select the degree rather than eyeballing it.
        </p>

        <h2>When to Use Polynomial Regression</h2>
        <p>
            Polynomial regression is useful when you have a rough idea of the functional form of your relationship (e.g., "this looks quadratic") and you want a simple, interpretable model. It's also a good stepping stone before reaching for more flexible nonlinear models like tree-based methods or neural networks.
        </p>
        <p>
            The main limitation is that it's sensitive to outliers and can behave erratically outside the range of your training data. If you need predictions far from your training distribution, be careful.
        </p>

        <p>
            The full notebook is on <a href="https://github.com/reubenl10/Machine-Learning-Algs-From-Scatch" style="color:#14b8a6;font-weight:600;">GitHub</a>.
        </p>
    </article>

    <footer>
        <p>&copy; 2026 Reuben Lopez · Houston, TX · rlacm10@icloud.com</p>
    </footer>

</body>
</html>
